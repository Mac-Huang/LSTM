{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022871,
     "end_time": "2024-03-27T04:04:10.273101",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.250230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Index\n",
    "\n",
    "* [Introduction](#1)\n",
    "* [Why Tokenization is Required?](#2)\n",
    "* [Tokenization Techniques](#3)\n",
    "  - [Tokenization Using Python's Inbuilt Method](#4)\n",
    "  - [Tokenization Using Regular Expressions(RegEx)](#5)\n",
    "  - [Tokenization Using NLTK](#6)\n",
    "  - [Tokenization Using spaCy](#7)\n",
    "  - [Tokenization using Keras](#8)\n",
    "  - [Tokenization using Gensim](#9)\n",
    "* [Conclusion](#10)\n",
    "* [References](#11)\n",
    "\n",
    "**Tutorial contains friendly description of multiple tokenization methods and python code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023336,
     "end_time": "2024-03-27T04:04:10.317280",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.293944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction <a id =\"1\"></a>\n",
    "\n",
    "Tokenization is one of the first step in any NLP pipeline. Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens. If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'. Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point and newline char are used for Sentence Tokenization.  We have to choose the appropriate method as per the task in hand. While performing the tokenization few characters like spaces, punctuations are ignored and will not be the part of final list of tokens.\n",
    "\n",
    "![NLP_Tokenization](https://raw.githubusercontent.com/satishgunjal/images/master/NLP_Tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020382,
     "end_time": "2024-03-27T04:04:10.359205",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.338823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why Tokenization is Required? <a id =\"2\"></a>\n",
    "Every sentence gets its meaning by the words present in it. So by analyzing the words present in the text we can easily interpret the meaning of the text. Once we have a list of words we can also use statistical tools and methods to get more insights into the text. For example, we can use word count and word frequency to find out important of word in that sentence or document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02059,
     "end_time": "2024-03-27T04:04:10.400815",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.380225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization Techniques <a id =\"3\"></a>\n",
    "There are multiple ways we can perform tokenization on given text data. We can choose any method based on language, library and purpose of modeling.\n",
    "\n",
    "## Tokenization Using Python's Inbuilt Method <a id =\"4\"></a>\n",
    "\n",
    "![NLP_Tokenization](https://raw.githubusercontent.com/satishgunjal/images/master/python_split_syntax.png)\n",
    "\n",
    "* We can use **split()** method to split a string into a list where each word is a list item.\n",
    "* By default split() use whitespace as separater, but we can change it to anything.\n",
    "\n",
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:10.446165Z",
     "iopub.status.busy": "2024-03-27T04:04:10.445335Z",
     "iopub.status.idle": "2024-03-27T04:04:10.452732Z",
     "shell.execute_reply": "2024-03-27T04:04:10.452072Z"
    },
    "papermill": {
     "duration": 0.031428,
     "end_time": "2024-03-27T04:04:10.452873",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.421445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge,', 'library', 'and', 'purpose', 'of', 'modeling.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "# Split text by whitespace\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021114,
     "end_time": "2024-03-27T04:04:10.496978",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.475864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Observe in above list, words like 'language,' and  'modeling.' are containing punctuation at the end of them. **Python split method do not consider punctuation as separate token.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020601,
     "end_time": "2024-03-27T04:04:10.538717",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.518116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:10.583883Z",
     "iopub.status.busy": "2024-03-27T04:04:10.583232Z",
     "iopub.status.idle": "2024-03-27T04:04:10.590279Z",
     "shell.execute_reply": "2024-03-27T04:04:10.590850Z"
    },
    "papermill": {
     "duration": 0.031385,
     "end_time": "2024-03-27T04:04:10.591013",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.559628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets split the given text by full stop (.)\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "text.split(\". \") # Note the space after the full stop makes sure that we dont get empty element at the end of list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021723,
     "end_time": "2024-03-27T04:04:10.634823",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.613100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see, split() since we can't use multiple separator split() method failed to split the last sentence from separator (!). We can overcome this drawback by applying split method multiple times with different separator but there are better ways to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021144,
     "end_time": "2024-03-27T04:04:10.677830",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.656686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization Using Regular Expressions(RegEx) <a id =\"5\"></a>\n",
    "\n",
    "![python_regex_syntax](https://raw.githubusercontent.com/satishgunjal/images/master/python_regex_syntax.png)\n",
    "\n",
    "* A regular expression is a sequence of characters that define a search pattern.\n",
    "* Using RegEx we can match character combinations in string and perform word/sentence tokenization.\n",
    "* Please refer [regex101](https://regex101.com/) for testing your regular expression syntax.\n",
    "* We can use Python's **re** library for RegeEx related operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021531,
     "end_time": "2024-03-27T04:04:10.720774",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.699243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:10.768005Z",
     "iopub.status.busy": "2024-03-27T04:04:10.767251Z",
     "iopub.status.idle": "2024-03-27T04:04:10.772777Z",
     "shell.execute_reply": "2024-03-27T04:04:10.773300Z"
    },
    "papermill": {
     "duration": 0.031091,
     "end_time": "2024-03-27T04:04:10.773455",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.742364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "tokens = re.findall(\"[\\w]+\", text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022453,
     "end_time": "2024-03-27T04:04:10.818362",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.795909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Based on RegEx pattern we are able to generate the list of words. Details about each character in our RegEx pattern is as below.\n",
    "```\n",
    "[] :\tA set of characters.\n",
    "\\w :    Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character).\n",
    "+  :\tOne or more occurrences.\n",
    "```\n",
    "\n",
    "So our RegEx pattern signifies that the code should find all the alphanumeric characters until any other character is encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024531,
     "end_time": "2024-03-27T04:04:10.864930",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.840399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:10.916034Z",
     "iopub.status.busy": "2024-03-27T04:04:10.915093Z",
     "iopub.status.idle": "2024-03-27T04:04:10.919807Z",
     "shell.execute_reply": "2024-03-27T04:04:10.919153Z"
    },
    "papermill": {
     "duration": 0.032608,
     "end_time": "2024-03-27T04:04:10.919932",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.887324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time',\n",
       " 'So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "tokens_sent = re.compile('[.!?] ').split(text) # Using compile method to combine RegEx patterns\n",
    "tokens_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024301,
     "end_time": "2024-03-27T04:04:10.967059",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.942758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see from above result, we are able to split sentence using multiple separators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022307,
     "end_time": "2024-03-27T04:04:11.012428",
     "exception": false,
     "start_time": "2024-03-27T04:04:10.990121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization Using NLTK <a id =\"6\"></a>\n",
    "* Natural Language Toolkit (NLTK) is library written in python for natural language processing.\n",
    "* NLTK has module **word_tokenize()** for word tokenization and **sent_tokenize()** for sentence tokenization.\n",
    "* Syntax to install NLTK is as below\n",
    "```\n",
    "!pip install --user -U nltk\n",
    "```\n",
    "* Note that we are going use \"!\" before the command to let notebook know that, it should read as commandline command\n",
    "\n",
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:11.062238Z",
     "iopub.status.busy": "2024-03-27T04:04:11.061437Z",
     "iopub.status.idle": "2024-03-27T04:04:22.724174Z",
     "shell.execute_reply": "2024-03-27T04:04:22.723376Z"
    },
    "papermill": {
     "duration": 11.688453,
     "end_time": "2024-03-27T04:04:22.724315",
     "exception": false,
     "start_time": "2024-03-27T04:04:11.035862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\r\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 11.3 MB/s \r\n",
      "\u001b[?25hCollecting regex>=2021.8.3\r\n",
      "  Downloading regex-2023.12.25-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 761 kB 67.0 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.45.0)\r\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.1.1)\r\n",
      "Requirement already satisfied, skipping upgrade: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (0.14.1)\r\n",
      "Installing collected packages: regex, nltk\r\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\r\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\r\n",
      "\r\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\r\n",
      "\r\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you'll have nltk 3.8.1 which is incompatible.\u001b[0m\r\n",
      "Successfully installed nltk-3.8.1 regex-2023.12.25\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:22.786112Z",
     "iopub.status.busy": "2024-03-27T04:04:22.785066Z",
     "iopub.status.idle": "2024-03-27T04:04:24.340211Z",
     "shell.execute_reply": "2024-03-27T04:04:24.339458Z"
    },
    "papermill": {
     "duration": 1.588542,
     "end_time": "2024-03-27T04:04:24.340346",
     "exception": false,
     "start_time": "2024-03-27T04:04:22.751804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026496,
     "end_time": "2024-03-27T04:04:24.393792",
     "exception": false,
     "start_time": "2024-03-27T04:04:24.367296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice that NLTK word tokenization also consider the punctuation as token. During text cleaning process we have to account for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026471,
     "end_time": "2024-03-27T04:04:24.447183",
     "exception": false,
     "start_time": "2024-03-27T04:04:24.420712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:24.510535Z",
     "iopub.status.busy": "2024-03-27T04:04:24.509495Z",
     "iopub.status.idle": "2024-03-27T04:04:24.515896Z",
     "shell.execute_reply": "2024-03-27T04:04:24.515195Z"
    },
    "papermill": {
     "duration": 0.041956,
     "end_time": "2024-03-27T04:04:24.516019",
     "exception": false,
     "start_time": "2024-03-27T04:04:24.474063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences.',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time!',\n",
       " 'So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026974,
     "end_time": "2024-03-27T04:04:24.570535",
     "exception": false,
     "start_time": "2024-03-27T04:04:24.543561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization Using spaCy <a id =\"7\"></a>\n",
    "* spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython\n",
    "* in spaCy we create language model object, which then used for word and sentence tokenization\n",
    "* Syntax to install spaCy library and English model is as below\n",
    "```\n",
    "!pip install spacy\n",
    "!python -m spacy download en\n",
    "```\n",
    "* Note that we are going use \"!\" before the command to let notebook know that, it should read as commandline command\n",
    "\n",
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:24.629536Z",
     "iopub.status.busy": "2024-03-27T04:04:24.628859Z",
     "iopub.status.idle": "2024-03-27T04:04:40.472042Z",
     "shell.execute_reply": "2024-03-27T04:04:40.470769Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 15.874181,
     "end_time": "2024-03-27T04:04:40.472182",
     "exception": false,
     "start_time": "2024-03-27T04:04:24.598001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (2.2.4)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.18.5)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (4.45.0)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.2)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.2)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.8.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.23.0)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.2)\r\n",
      "Requirement already satisfied: thinc==7.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (7.4.0)\r\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.4.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /opt/conda/lib/python3.7/site-packages (2.2.5)\r\n",
      "Requirement already satisfied: spacy>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\r\n",
      "Requirement already satisfied: thinc==7.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.45.0)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\r\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_sm')\r\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/en_core_web_sm -->\r\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/data/en\r\n",
      "You can now load the model via spacy.load('en')\r\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:40.546067Z",
     "iopub.status.busy": "2024-03-27T04:04:40.545276Z",
     "iopub.status.idle": "2024-03-27T04:04:41.148071Z",
     "shell.execute_reply": "2024-03-27T04:04:41.147458Z"
    },
    "papermill": {
     "duration": 0.644078,
     "end_time": "2024-03-27T04:04:41.148208",
     "exception": false,
     "start_time": "2024-03-27T04:04:40.504130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load English model from spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer. \n",
    "# nlp object will be used to create 'doc' object which uses preprecoessing pipeline's components such as tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "\n",
    "# Now we will process above text using 'nlp' object. Which is use to create documents with linguistic annotations and various nlp properties\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Above step has already tokenized our text but its in doc format, so lets write fo loop to create list of it\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031118,
     "end_time": "2024-03-27T04:04:41.211090",
     "exception": false,
     "start_time": "2024-03-27T04:04:41.179972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:41.279042Z",
     "iopub.status.busy": "2024-03-27T04:04:41.278314Z",
     "iopub.status.idle": "2024-03-27T04:04:41.485166Z",
     "shell.execute_reply": "2024-03-27T04:04:41.485707Z"
    },
    "papermill": {
     "duration": 0.243244,
     "end_time": "2024-03-27T04:04:41.485865",
     "exception": false,
     "start_time": "2024-03-27T04:04:41.242621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Characters like periods, exclamation point and newline char are used to separate the sentences.', 'But one drawback with split() method, that we can only use one separator at a time!', 'So sentence tonenization wont be foolproof with split() method.']\n"
     ]
    }
   ],
   "source": [
    "# Load English tokenizer, tager, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Add component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "\n",
    "# nlp object is used to create documents with linguistic annotations\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create list of sentence tokens\n",
    "\n",
    "sentence_list =[]\n",
    "for sentence in doc.sents:\n",
    "    sentence_list.append(sentence.text)\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03252,
     "end_time": "2024-03-27T04:04:41.550543",
     "exception": false,
     "start_time": "2024-03-27T04:04:41.518023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization using Keras <a id =\"8\"></a>\n",
    "* Keras is opensource neural network library written in python. It is easy to use and it is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML\n",
    "* To perform word tokenization we use the **text_to_word_sequence()** method from the **keras.preprocessing.text class**\n",
    "* By default, this function automatically does 3 things:\n",
    "    * Splits words by space (split=” “).\n",
    "    * Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’).\n",
    "    * Converts text to lowercase (lower=True).\n",
    "* Syntx to install Keras\n",
    "```\n",
    "!pip install Keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:41.626565Z",
     "iopub.status.busy": "2024-03-27T04:04:41.625413Z",
     "iopub.status.idle": "2024-03-27T04:04:48.703896Z",
     "shell.execute_reply": "2024-03-27T04:04:48.704726Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 7.122481,
     "end_time": "2024-03-27T04:04:48.705062",
     "exception": false,
     "start_time": "2024-03-27T04:04:41.582581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras in /opt/conda/lib/python3.7/site-packages (2.4.3)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from Keras) (2.10.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from Keras) (5.3.1)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from Keras) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from Keras) (1.18.5)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->Keras) (1.14.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033649,
     "end_time": "2024-03-27T04:04:48.774006",
     "exception": false,
     "start_time": "2024-03-27T04:04:48.740357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:48.848591Z",
     "iopub.status.busy": "2024-03-27T04:04:48.847604Z",
     "iopub.status.idle": "2024-03-27T04:04:54.931572Z",
     "shell.execute_reply": "2024-03-27T04:04:54.930901Z"
    },
    "papermill": {
     "duration": 6.123498,
     "end_time": "2024-03-27T04:04:54.931836",
     "exception": false,
     "start_time": "2024-03-27T04:04:48.808338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'we', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "\n",
    "tokens = text_to_word_sequence(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035205,
     "end_time": "2024-03-27T04:04:55.002144",
     "exception": false,
     "start_time": "2024-03-27T04:04:54.966939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can notice, all words are also converted to lowercase. This is default behavior we can change it by changing the arguments e.g. text_to_word_sequence(text,lower=False)\n",
    "\n",
    "### Sentence Tokenization\n",
    "For sentence tokenization we can use filters like \"!.\\n\" to split the text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:55.077095Z",
     "iopub.status.busy": "2024-03-27T04:04:55.076138Z",
     "iopub.status.idle": "2024-03-27T04:04:55.081701Z",
     "shell.execute_reply": "2024-03-27T04:04:55.081077Z"
    },
    "papermill": {
     "duration": 0.045731,
     "end_time": "2024-03-27T04:04:55.081838",
     "exception": false,
     "start_time": "2024-03-27T04:04:55.036107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " ' but one drawback with split() method, that we can only use one separator at a time',\n",
       " ' so sentence tonenization wont be foolproof with split() method']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "\n",
    "text_to_word_sequence(text, split= \".\", filters=\"!.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035748,
     "end_time": "2024-03-27T04:04:55.151746",
     "exception": false,
     "start_time": "2024-03-27T04:04:55.115998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization using Gensim <a id =\"9\"></a>\n",
    "* Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\n",
    "* We are going to use **tokenize()** from **gensim.utility** class for word tokenization.\n",
    "* Unlike other libraries Gensim has separate method **split_sentences()** from class **gensim.summarization.textcleaner** for sentence tokenization. \n",
    "* Syntx to install Gensim\n",
    "```\n",
    "!pip install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-03-27T04:04:55.233165Z",
     "iopub.status.busy": "2024-03-27T04:04:55.232007Z",
     "iopub.status.idle": "2024-03-27T04:05:02.326750Z",
     "shell.execute_reply": "2024-03-27T04:05:02.326114Z"
    },
    "papermill": {
     "duration": 7.141009,
     "end_time": "2024-03-27T04:05:02.326895",
     "exception": false,
     "start_time": "2024-03-27T04:04:55.185886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (3.8.3)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (2.1.1)\r\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.18.5)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.4.1)\r\n",
      "Requirement already satisfied: boto in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.14.56)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\r\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.56 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.56)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\r\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.56->boto3->smart-open>=1.8.1->gensim) (0.15.2)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.56->boto3->smart-open>=1.8.1->gensim) (2.8.1)\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036009,
     "end_time": "2024-03-27T04:05:02.400085",
     "exception": false,
     "start_time": "2024-03-27T04:05:02.364076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:05:02.482413Z",
     "iopub.status.busy": "2024-03-27T04:05:02.481400Z",
     "iopub.status.idle": "2024-03-27T04:05:02.755878Z",
     "shell.execute_reply": "2024-03-27T04:05:02.756590Z"
    },
    "papermill": {
     "duration": 0.320242,
     "end_time": "2024-03-27T04:05:02.756802",
     "exception": false,
     "start_time": "2024-03-27T04:05:02.436560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "\n",
    "tokens = list(tokenize(text))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:05:02.837082Z",
     "iopub.status.busy": "2024-03-27T04:05:02.835977Z",
     "iopub.status.idle": "2024-03-27T04:05:02.843539Z",
     "shell.execute_reply": "2024-03-27T04:05:02.842940Z"
    },
    "papermill": {
     "duration": 0.047509,
     "end_time": "2024-03-27T04:05:02.843695",
     "exception": false,
     "start_time": "2024-03-27T04:05:02.796186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['characters like periods', ' exclamation point and newline char are used to separate the sentences', ' but one drawback with split', ' method', ' that we can only use one separator at a time', ' so sentence tonenization wont be fullproof with split', ' method']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be fullproof with split() method.\"\"\"\n",
    "\n",
    "tokens = text_to_word_sequence(text, split= \".\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036703,
     "end_time": "2024-03-27T04:05:02.917963",
     "exception": false,
     "start_time": "2024-03-27T04:05:02.881260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T04:05:03.007991Z",
     "iopub.status.busy": "2024-03-27T04:05:03.007020Z",
     "iopub.status.idle": "2024-03-27T04:05:03.010745Z",
     "shell.execute_reply": "2024-03-27T04:05:03.011386Z"
    },
    "papermill": {
     "duration": 0.054077,
     "end_time": "2024-03-27T04:05:03.011558",
     "exception": false,
     "start_time": "2024-03-27T04:05:02.957481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences.',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time!',\n",
       " 'So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization.textcleaner import split_sentences\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "\n",
    "list(split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046585,
     "end_time": "2024-03-27T04:05:03.102181",
     "exception": false,
     "start_time": "2024-03-27T04:05:03.055596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion <a id =\"10\"></a>\n",
    "There are multiple ways to do the tokenization. We can use any library depending on our requirement and features supported by the library. Feel free to try above code with different text snippet to get hold of how tokenization work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037895,
     "end_time": "2024-03-27T04:05:03.177524",
     "exception": false,
     "start_time": "2024-03-27T04:05:03.139629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References <a id =\"11\"></a>\n",
    "* https://keras.io/api/preprocessing/text/#text_to_word_sequence\n",
    "* https://www.nltk.org/\n",
    "* https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "* https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4\n",
    "* https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    },
    {
     "datasetId": 11827,
     "sourceId": 16290,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 39657,
     "sourceId": 61725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 46601,
     "sourceId": 84740,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 100982,
     "sourceId": 239192,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 30764,
     "sourceId": 533474,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 505302,
     "sourceId": 934476,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 863934,
     "sourceId": 1472453,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 519753,
     "sourceId": 1640141,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 55151,
     "sourceId": 2669146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30008,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 57.617327,
   "end_time": "2024-03-27T04:05:03.323568",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-27T04:04:05.706241",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

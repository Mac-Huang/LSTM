{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "### TORCHTEXT HAS ALREADY BEEN OBSOLETE!!!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to LSTM using Text Sentiment Analysis as an Example\n",
    "\n",
    "Sentiment analysis of text, also called opinion extraction, topic analysis, or sentiment classification, is commonly used in everyday applications.\n",
    "\n",
    "For instance, when shopping on an e-commerce website, we often review the product ratings to see if there are negative reviews. These review texts express different emotions and attitudes, such as happiness, anger, sadness, praise, and criticism.\n",
    "\n",
    "In such a scenario, the computer automatically categorizes these reviews as positive, neutral, or negative. The technology behind this categorization is sentiment analysis.\n",
    "\n",
    "Moreover, upon further observation, you'll notice labels such as \"sound volume is appropriate,\" \"fast connection speed,\" or \"excellent customer service.\" These labels are also extracted automatically by the computer, identifying topics or opinions based on the text.\n",
    "\n",
    "The rapid growth of sentiment analysis has been greatly aided by the rise of social media. Since the early 2000s, sentiment analysis has become one of the most active areas of research in natural language processing (NLP). It is widely applied in personalized recommendations, business decision-making, and public opinion monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Preparation\n",
    "\n",
    "We have a set of movie review data (IMDB dataset) categorized into two types: positive reviews and negative reviews. Our goal is to train a sentiment analysis model that can classify the review texts.\n",
    "\n",
    "In essence, this is a text classification problem, specifically a binary classification task, focused on movie review texts. Let's look at the training data.\n",
    "\n",
    "IMDB (Internet Movie Database) is a dataset with 50,000 highly polarized movie reviews. It is divided into training and testing sets, each containing 25,000 reviews. Both sets include 50% positive and 50% negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using Torchtext to Load the Dataset\n",
    "\n",
    "First, install the required package:\n",
    "\n",
    "```bash\n",
    "pip install torchtext\n",
    "```\n",
    "\n",
    "Torchtext provides the IMDB dataset along with functionalities like loading corpora, converting words to vectors, mapping words to indices, and creating iteratorsâ€”all essential for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the IMDB dataset\n",
    "import torchtext\n",
    "train_iter = torchtext.datasets.IMDB(root='./data', split='train')\n",
    "# Each line contains the sentiment label followed by the review text.\n",
    "# \"neg\" indicates negative, and \"pos\" indicates positive.\n",
    "print(next(iter(train_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Pipelines\n",
    "\n",
    "After loading the dataset, we need to convert the text and labels into vectors that the computer can read. Typically, this involves tokenizing the text and mapping the words to IDs.\n",
    "\n",
    "Torchtext provides basic text processing tools, including the tokenizer and vocabulary functions. The `get_tokenizer` function creates a tokenizer, while the `build_vocab_from_iterator` function builds the vocabulary using the training data iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# ['here', 'is', 'the', 'an', 'example', '!']\n",
    "print(tokenizer('here is the an example!'))\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# [131, 9, 40, 464, 0, 0]\n",
    "print(vocab(tokenizer('here is the an example <pad> <pad>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the vocabulary creation process, the `yield_tokens` function tokenizes each data point in the training dataset. The user can customize special tokens such as `<pad>` for padding and `<unk>` for unknown words.\n",
    "\n",
    "Next, we create data processing pipelines. The `text_pipeline` converts a given text into token IDs, while the `label_pipeline` maps sentiment labels to numerical values: \"neg\" to 0 and \"pos\" to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing pipelines\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: 1 if x == 'pos' else 0\n",
    "\n",
    "# [131, 9, 40, 464, 0, 0, ... , 0]\n",
    "print(text_pipeline('here is the an example'))\n",
    "\n",
    "# Output: 0\n",
    "print(label_pipeline('neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training Data\n",
    "\n",
    "Once we have the data pipelines, we can generate the training data using the `DataLoader`.\n",
    "\n",
    "Due to variable-length sentences in the dataset, we need to pad or truncate each sequence to a fixed length. For instance, if the maximum sentence length is 256 words, longer sentences will be truncated, and shorter ones will be padded.\n",
    "\n",
    "All these operations can be handled by the `collate_batch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    max_length = 256\n",
    "    pad = text_pipeline('<pad>')\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = text_pipeline(_text)[:max_length]\n",
    "         length_list.append(len(processed_text))\n",
    "         text_list.append((processed_text + pad * max_length)[:max_length])\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.tensor(text_list, dtype=torch.int64)\n",
    "    length_list = torch.tensor(length_list, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device), length_list.to(device)\n",
    "\n",
    "train_iter = torchtext.datasets.IMDB(root='./data', split='train')\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "train_dataloader = DataLoader(split_train_, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow includes:\n",
    "\n",
    "1. Loading the IMDB training dataset using Torchtext.\n",
    "2. Converting the iterator to `Dataset` type.\n",
    "3. Splitting the dataset into training (95%) and validation (5%) sets.\n",
    "4. Creating `DataLoader` for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "\n",
    "Due to inherent limitations of RNNs, such as gradient vanishing or exploding during backpropagation, the LSTM network improves on the RNN by using gates to combine short-term and long-term memory, thereby mitigating these issues.\n",
    "\n",
    "#### Why predict with hidden state?\n",
    "The hidden state is used for predictions in LSTMs because it provides the most up-to-date representation of the input sequence, combining both long-term and short-term information. Unlike the cell state, which stores raw long-term memory, the hidden state is refined by the output gate to represent relevant features for prediction, making it a comprehensive and output-ready summary of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, pad_index=0):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, ids, length):\n",
    "        # Apply embedding layer to input ids to get word embeddings, followed by dropout for regularization\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        \n",
    "        # Pack the padded sequence to *ignore padding tokens* during LSTM processing, using actual sequence lengths\n",
    "        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass the packed sequence through the LSTM layer\n",
    "        # packed_output contains the output for each time step, while hidden and cell contain the final states\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        # Unpack the packed sequence back to padded form, used for further processing if needed\n",
    "        output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        # Extract the final hidden state\n",
    "        # If the LSTM is bidirectional, concatenate the final states from both forward and backward passes\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "        \n",
    "        # Pass the hidden state through the fully connected layer to produce the final output prediction\n",
    "        prediction = self.fc(hidden)\n",
    "        \n",
    "        # Return the final prediction\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "Finally, we train and evaluate the LSTM model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "output_dim = 2\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout_rate = 0.5\n",
    "lr = 5e-4\n",
    "\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
    "        (label, ids, length) = batch\n",
    "        label = label.to(device)\n",
    "        ids = ids.to(device)\n",
    "        length = length.to(device)\n",
    "        prediction = model(ids, length)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    return epoch_losses, epoch_accs\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
    "            (label, ids, length) = batch\n",
    "            label = label.to(device)\n",
    "            ids = ids.to(device)\n",
    "            length = length.to(device)\n",
    "            prediction = model(ids, length)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "    return epoch_losses, epoch_accs\n",
    "\n",
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy\n",
    "\n",
    "n_epochs = 10\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
    "    valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n",
    "    train_losses.extend(train_loss)\n",
    "    train_accs.extend(train_acc)\n",
    "    valid_losses.extend(valid_loss)\n",
    "    valid_accs.extend(valid_acc)\n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    epoch_train_acc = np.mean(train_acc)\n",
    "    epoch_valid_loss = np.mean(valid_loss)\n",
    "    epoch_valid_acc = np.mean(valid_acc)\n",
    "    if epoch_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss\n",
    "        torch.save(model.state_dict(), 'lstm.pt')\n",
    "    print(f'epoch: {epoch + 1}')\n",
    "    print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
    "    print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
